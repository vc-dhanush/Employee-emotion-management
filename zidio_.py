# -*- coding: utf-8 -*-
"""ZIDIO     .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r3CQkQwliBq7ACQ5QJMJfWD6uhYIDRrW
"""

pip install transformers torch torchvision opencv-python fer librosa soundfile numpy pandas

pip install gradio

!pip install resampy

pip install gradio transformers deepface opencv-python librosa scikit-learn pandas matplotlib

pip install deepface

import os
import zipfile
import librosa
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report
import joblib
import requests
from tqdm import tqdm

# ----------------------------------------
# 1. Download RAVDESS Dataset
# ----------------------------------------
def download_ravdess():
    url = "https://zenodo.org/record/1188976/files/Audio_Speech_Actors_01-24.zip"
    filename = "ravdess.zip"

    if not os.path.exists("ravdess"):
        print("Downloading RAVDESS...")
        with requests.get(url, stream=True) as r:
            r.raise_for_status()
            with open(filename, 'wb') as f:
                for chunk in tqdm(r.iter_content(chunk_size=8192)):
                    f.write(chunk)

        print("Unzipping...")
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall("ravdess")
        os.remove(filename)

# ----------------------------------------
# 2. Feature Extraction (no resampy)
# ----------------------------------------
def extract_features(file_path):
    y, sr = librosa.load(file_path, sr=None)
    if len(y) < sr * 3:
        y = np.pad(y, (0, sr * 3 - len(y)))
    y = y[:sr * 3]
    mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
    return mfcc

# ----------------------------------------
# 3. Load Data
# ----------------------------------------
def load_data():
    emotions = {
        '01': 'neutral',
        '02': 'calm',
        '03': 'happy',
        '04': 'sad',
        '05': 'angry',
        '06': 'fearful',
        '07': 'disgust',
        '08': 'surprised'
    }

    X, y = [], []
    for root, _, files in os.walk("ravdess"):
        for file in files:
            if file.endswith(".wav"):
                emotion_code = file.split("-")[2]
                emotion = emotions.get(emotion_code)
                if emotion:
                    path = os.path.join(root, file)
                    features = extract_features(path)
                    X.append(features)
                    y.append(emotion)
    return np.array(X), np.array(y)

# ----------------------------------------
# 4. Train Model and Save
# ----------------------------------------
def train_model():
    download_ravdess()
    X, y = load_data()

    le = LabelEncoder()
    y_encoded = le.fit_transform(y)

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)

    model = RandomForestClassifier(n_estimators=200, random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    print("\nüéØ Classification Report:")
    print(classification_report(y_test, y_pred, target_names=le.classes_))

    joblib.dump(model, "speech_emotion_model.pkl")
    joblib.dump(scaler, "scaler.pkl")
    print("‚úÖ Model and scaler saved!")

# ----------------------------------------
# Run
# ----------------------------------------
if __name__ == "__main__":
    train_model()

import gradio as gr
import librosa
import numpy as np
import pandas as pd
import os
from datetime import datetime
from transformers import pipeline
from deepface import DeepFace


text_classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base")


def detect_text_emotion(text):
    result = text_classifier(text)
    return result[0]['label']

def extract_audio_features(file_path):
    y, sr = librosa.load(file_path)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)
    return np.mean(mfcc.T, axis=0)

def detect_audio_emotion(audio_path):
    features = extract_audio_features(audio_path)
    return "happy" if np.mean(features) > 0 else "sad"


def detect_image_emotion(image_path):
    try:
        analysis = DeepFace.analyze(img_path=image_path, actions=['emotion'], enforce_detection=False)
        return analysis[0]['dominant_emotion']
    except Exception as e:
        return "Error analyzing image"


def recommend_task(emotion):
    mapping = {
        "happy": "Work on creative projects",
        "sad": "Take a short break or do light tasks",
        "angry": "Avoid team meetings, focus on solo tasks",
        "neutral": "Proceed with daily assigned tasks",
        "fear": "Engage in supportive team discussions",
        "disgust": "Review and restructure task workload",
        "surprise": "Participate in team idea sharing"
    }
    return mapping.get(emotion.lower(), "Do general tasks")


def log_and_alert(user, emotion):
    df = pd.DataFrame([[user, emotion, datetime.now()]], columns=["User", "Emotion", "Timestamp"])
    df.to_csv("mood_logs.csv", mode='a', header=not os.path.exists("mood_logs.csv"), index=False)

    if os.path.exists("mood_logs.csv"):
        logs = pd.read_csv("mood_logs.csv")
        recent = logs.tail(3)
        if len(recent) >= 3 and all(recent["Emotion"].str.lower() == emotion.lower()):
            return f"‚ö†Ô∏è HR ALERT: Prolonged '{emotion}' detected!"
    return ""


def handle_text_input(text):
    emotion = detect_text_emotion(text)
    task = recommend_task(emotion)
    alert = log_and_alert("Employee1", emotion)
    return emotion, task, alert

def handle_audio_input(audio_path):
    emotion = detect_audio_emotion(audio_path)
    task = recommend_task(emotion)
    alert = log_and_alert("Employee1", emotion)
    return emotion, task, alert

def handle_image_input(image_path):
    emotion = detect_image_emotion(image_path)
    task = recommend_task(emotion)
    alert = log_and_alert("Employee1", emotion)
    return emotion, task, alert


with gr.Blocks() as app:
    gr.Markdown("# ü§ñ Zidio AI-Powered Task Optimizer")
    gr.Markdown("Analyze emotions from text, audio, and images. Get smart task suggestions and HR alerts.")

    with gr.Row():
        with gr.Column():
            text_input = gr.Textbox(label="Enter Employee's Text Input")
            text_btn = gr.Button("Analyze Text")
        with gr.Column():
            audio_input = gr.Audio(type="filepath", label="Upload Audio (.wav)")
            audio_btn = gr.Button("Analyze Audio")
        with gr.Column():
            image_input = gr.Image(type="filepath", label="Upload Face Image")
            image_btn = gr.Button("Analyze Image")

    output_emotion = gr.Text(label="Detected Emotion")
    output_task = gr.Text(label="Recommended Task")
    output_alert = gr.Text(label="HR Alert (if any)")

    text_btn.click(fn=handle_text_input, inputs=text_input, outputs=[output_emotion, output_task, output_alert])
    audio_btn.click(fn=handle_audio_input, inputs=audio_input, outputs=[output_emotion, output_task, output_alert])
    image_btn.click(fn=handle_image_input, inputs=image_input, outputs=[output_emotion, output_task, output_alert])

app.launch()